{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Natural language processing to analyze the abstracts from a scientific conference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import mpld3\n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract the text of the abstracts at the conference\n",
    "I think a better solution is to copy everything into a text file, and then follow a simple chain of logic to parse it.\n",
    "\n",
    "- Every ID number comes IN ORDER in the file.\n",
    "- Every ID number comes after a new line and is followed by a space.\n",
    "- So if we scan line by line, and incriment the ID we are looking for, then it is really unlikely that we will run into any conflicts (i.e. next ID is 100 and 100 is used in the text of 99, causing it to be split.)  \n",
    "- Finally, we want to ommit the author line for each abstract. This will work since the author line and the text line are seperate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844 abtracts\n"
     ]
    }
   ],
   "source": [
    "with open(\"dros_2019_data/Dros19_Abstracts_Full_v2_hard-Copy.txt\", 'r') as f:\n",
    "    \n",
    "    abstracts = {}\n",
    "    idfind = 2\n",
    "    record = ''\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        if (str(idfind + 1) + ' ') in line:\n",
    "            abstracts[idfind] = record\n",
    "            record = ''\n",
    "            idfind += 1\n",
    "        record += line\n",
    "            \n",
    "print(len(abstracts), 'abtracts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Web Scraping the Dros 2019 abstract assignment data\n",
    "The data for abstracts at this conference comes in two parts - first, the PDF that actually has all the abstracts in them. However, that PDF doesn't have the abstract assignment (poster or talk). For the assignment, we need the conference web page: http://conferences.genetics-gsa.org/drosophila/2019/assignments  \n",
    "  \n",
    "The assignments are in a large table, that is thankfully continuous over the whole page. The table has the presenter's name, partial abstract title, session assignment, program #, session date, and presentation time. We need hte session assignment and the program number. The number will allow us to connect it to the correct abstract in the program.  \n",
    "  \n",
    "I've saved the web page in this folder in case it changes or gets removed. At this point it's not going to change anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we tell BeautifulSoup and tell it which parser to use\n",
    "soup = BeautifulSoup(open(\"dros_2019_data/abstract_lookup.html\"), \"html.parser\")\n",
    "\n",
    "# make a pandas data frame from the table on the website\n",
    "ab_table = soup.find(id=\"authassign-All\")\n",
    "abstracts_df = pd.read_html(str(ab_table))[0]\n",
    "\n",
    "# group the assignments together\n",
    "assignments = {}\n",
    "\n",
    "for idx, num in enumerate(abstracts_df['Program #'].values):\n",
    "    if \"P\" not in num:\n",
    "  \n",
    "        if \"Poster\" in abstracts_df['Session Assignment'].values[idx]:\n",
    "            assignments[num] = {\n",
    "                'type' : 'poster',\n",
    "                'section' : abstracts_df['Session Assignment'].values[idx].split('Poster')[1].lower()\n",
    "            }\n",
    "\n",
    "        if \"Plenary\" in abstracts_df['Session Assignment'].values[idx]:\n",
    "            assignments[num] = {\n",
    "                'type' : 'talk',\n",
    "                'section' : abstracts_df['Session Assignment'].values[idx][7:].lower()\n",
    "            }\n",
    "\n",
    "        if \"Platform\" in abstracts_df['Session Assignment'].values[idx]:\n",
    "            assignments[num] = {\n",
    "                'type' : 'talk',\n",
    "                'section' : abstracts_df['Session Assignment'].values[idx].split('Platform')[1].lower()\n",
    "            }\n",
    "\n",
    "sections = set()\n",
    "for assign in assignments:\n",
    "    sections.add(assignments[assign]['section'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many secitons, and several fall into the same general category of study. So, I've made these group categories to narrow down the general thing that is being studied. Later we can try plotting by sections and by groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_groups = {\n",
    "    'cell biology: cytoskeleton, organelles and trafficking' : 'cell bio',\n",
    "    'cell biology: cytoskeleton, organelles, trafficking' : 'cell bio',\n",
    "    'cell death and cell stress' : 'cell bio',\n",
    "    'cell division and cell growth' : 'cell bio',\n",
    "    'cell division and growth control' : 'cell bio',\n",
    "    'cell stress and cell death' : 'cell bio',\n",
    "    'chromatin, epigenetics and genomics' : 'gene expression',\n",
    "    'chromatin, epigenetics and genomics ii' : 'gene expression',\n",
    "    'education ' : 'education',\n",
    "    'educational initiatives' : 'education',\n",
    "    'evolution' : 'evolution',\n",
    "    'evolution i' : 'evolution',\n",
    "    'evolution ii' : 'evolution',\n",
    "    'immunity and the microbiome' : 'models of human disease',\n",
    "    'models of human disease' : 'models of human disease',\n",
    "    'models of human disease i' : 'models of human disease',\n",
    "    'models of human disease ii' : 'models of human disease',\n",
    "    'neural circuits and behavior' : 'neurobiology',\n",
    "    'neural circuits and behavior i' : 'neurobiology',\n",
    "    'neural development and physiology' : 'neurobiology',\n",
    "    'neural development and physiology ii/neural circuits and behavior ii'  : 'neurobiology',\n",
    "    'patterning, morphogenesis and organogenesis' : 'developmental biology',\n",
    "    'patterning, morphogenesis and organogenesis i' : 'developmental biology',\n",
    "    'patterning, morphogenesis and organogenesis ii' : 'developmental biology',\n",
    "    'physiology, metabolism and aging' : 'developmental biology',\n",
    "    'physiology, metabolism and aging i' : 'developmental biology',\n",
    "    'physiology, metabolism and aging ii' : 'developmental biology',\n",
    "    'plenary ii' : 'plenary',\n",
    "    'plenary session i' : 'plenary',\n",
    "    'regulation of gene expression' : 'gene expression',\n",
    "    'regulation of gene expression i' : 'gene expression',\n",
    "    'regulation of gene expression ii/ chromatin, epigenetics and genomics i' : 'gene expression',\n",
    "    'reproduction and gametogenesis'  : 'developmental biology',\n",
    "    'signal transduction'  : 'developmental biology',\n",
    "    'stem cells, regeneration and tissue injury'  : 'developmental biology',\n",
    "    'techniques & technology' : 'education'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844 assignments\n"
     ]
    }
   ],
   "source": [
    "groups = set()\n",
    "for assign in assignments:\n",
    "    assignments[assign]['group'] = section_groups[assignments[assign]['section']]\n",
    "    groups.add(assignments[assign]['group'])\n",
    "    \n",
    "print(len(assignments), 'assignments')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are 844 abstracts and 844 assignments! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Some basic NLP investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to turn the abstracts into objects that NLTK can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: 'U' mode is deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "f = open(\"dros_2019_data/Dros19_Abstracts_Full_v2_hard-Copy.txt\", 'rU')\n",
    "text = f.read().split()\n",
    "full = nltk.Text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 14 of 14 matches:\n",
      "del of Huntington’s disease that demonstrates “prion-like” spreading of mutant\n",
      "ion of mutations that cause bias demonstrates that niche competition is geneti\n",
      "work for GWAS power analysis and demonstrates the feasibility of the Hybrid Sw\n",
      "erimental design and methodology demonstrates a novel technique for characteri\n",
      "a virus infected D. melanogaster demonstrates the presence of Vago, Vir-1, and\n",
      "wever, our work mutating H3.2K36 demonstrates that this modification is neithe\n",
      "ced neuromuscular function, this demonstrates that the VHD domain is critical \n",
      " cohabitation. This cohabitation demonstrates the evolution of linguistic vari\n",
      "tivation of different body parts demonstrates that changing the amount of sens\n",
      "through utilization of our assay demonstrates that three distinct responses ar\n",
      "tested in Drosophila. This study demonstrates the value of Drosophila in funct\n",
      "d lung tumours. Preliminary data demonstrates that synthetic lethal screening \n",
      "rotein. Taken together, our work demonstrates the efficacy and potential of th\n",
      "cesses. Collectively, this study demonstrates that both the non-nuclear and th\n"
     ]
    }
   ],
   "source": [
    "# check that it works by looking for a word\n",
    "full.concordance('demonstrates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggests indicates and showed in is indicate demonstrated demonstrate\n",
      "shows of how at for suggest identified with on via neuronal\n"
     ]
    }
   ],
   "source": [
    "# words that people use like demonstrate\n",
    "full.similar('demonstrates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 32401 samples and 257957 outcomes>\n",
      "[('Drosophila', 1369), ('cell', 925), ('gene', 666), ('cells', 661), ('expression', 639), ('University', 622), ('genes', 608), ('role', 522), ('Department', 496), ('also', 492), ('protein', 490), ('found', 439), ('flies', 421), ('genetic', 419), ('function', 407), ('results', 371), ('signaling', 369), ('proteins', 364), ('using', 342), ('identified', 332), ('two', 319), ('required', 304), ('model', 302), ('mutant', 292), ('neurons', 292), ('may', 283), ('show', 275), ('different', 273), ('melanogaster', 260), ('data', 258), ('fly', 257), ('loss', 253), ('human', 252), ('mechanisms', 247), ('specific', 245), ('complex', 243), ('development', 242), ('activity', 241), ('known', 235), ('studies', 234), ('changes', 233), ('regulation', 232), ('transcription', 229), ('levels', 229), ('within', 227), ('study', 227), ('identify', 227), ('including', 224), ('associated', 221), ('response', 221)]\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "frequency_dist = nltk.FreqDist(full)\n",
    "print(frequency_dist)\n",
    "\n",
    "# check the 200 most common words and keep all that are not stop words\n",
    "not_stops = []\n",
    "for word in frequency_dist.most_common(200):\n",
    "    if (str.isalpha(word[0])) and (word[0].lower() not in stopwords):\n",
    "        not_stops.append(word)\n",
    "\n",
    "# and print the 50 most common words\n",
    "print(not_stops[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually kind of cool. \"Drosophila\" comes out on top, which kind of makes sense. \"University\" and \"Department\" probably come from the author line.  \n",
    "  \n",
    "OK, first question. Split the data into two sets - one that got talks, and one that got posters, and see what the top 50 words are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "talks = ''\n",
    "posters = ''\n",
    "for x in abstracts:\n",
    "    if str(x) in assignments:\n",
    "        if assignments[str(x)]['type'] == 'talk':\n",
    "            talks += abstracts[x] + ' '\n",
    "        if assignments[str(x)]['type'] == 'poster':\n",
    "            posters += abstracts[x] + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talks\n",
      "{'regulation', 'results', 'University', 'mutant', 'DNA', 'also', 'data', 'protein', 'using', 'neurons', 'changes', 'suggest', 'signaling', 'cells', 'activity', 'found', 'specific', 'transcription', 'genetic', 'gene', 'human', 'flies', 'levels', 'pathway', 'factor', 'required', 'Drosophila', 'response', 'function', 'model', 'conserved', 'complex', 'Medical', 'cell', 'show', 'stem', 'identified', 'adult', 'expression', 'fly', 'Department', 'mechanisms', 'proteins', 'early', 'genes', 'growth', 'two', 'loss', 'different', 'role'}\n",
      "\n",
      "\n",
      "Posters\n",
      "{'results', 'University', 'mutant', 'also', 'data', 'protein', 'using', 'neurons', 'studies', 'suggest', 'signaling', 'cells', 'activity', 'found', 'specific', 'used', 'within', 'genetic', 'known', 'gene', 'human', 'identify', 'may', 'flies', 'levels', 'required', 'Drosophila', 'response', 'function', 'model', 'development', 'complex', 'cell', 'show', 'associated', 'identified', 'expression', 'fly', 'Department', 'mechanisms', 'including', 'proteins', 'genes', 'well', 'melanogaster', 'two', 'different', 'loss', 'study', 'role'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def top_fifty(corpus):\n",
    "    frequency_dist = nltk.FreqDist(corpus)\n",
    "\n",
    "    # check the 200 most common words and keep all that are not stop words\n",
    "    not_stops = []\n",
    "    for word in frequency_dist.most_common(200):\n",
    "        if (str.isalpha(word[0])) and (word[0].lower() not in stopwords):\n",
    "            not_stops.append(word)\n",
    "\n",
    "    # and print the 50 most common words\n",
    "    return [x[0] for x in not_stops][:50]\n",
    "\n",
    "talks1 = nltk.Text(talks.split())\n",
    "posters1 = nltk.Text(posters.split())\n",
    "\n",
    "print('Talks')\n",
    "top50_talks = set(top_fifty(talks1))\n",
    "print(top50_talks)\n",
    "print('\\n')\n",
    "\n",
    "print('Posters')\n",
    "top50_posters = set(top_fifty(posters1))\n",
    "print(top50_posters)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, they look pretty similar. Lets use some set joining functions to see what the union and differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common to both {'results', 'University', 'mutant', 'also', 'data', 'protein', 'using', 'neurons', 'suggest', 'signaling', 'cells', 'activity', 'found', 'specific', 'genetic', 'gene', 'human', 'flies', 'levels', 'required', 'Drosophila', 'response', 'function', 'model', 'complex', 'cell', 'show', 'identified', 'expression', 'fly', 'Department', 'mechanisms', 'proteins', 'genes', 'two', 'different', 'loss', 'role'} \n",
      "\n",
      "More common in talks {'regulation', 'pathway', 'factor', 'early', 'conserved', 'growth', 'DNA', 'transcription', 'Medical', 'stem', 'changes', 'adult'} \n",
      "\n",
      "More common in posters {'including', 'well', 'associated', 'development', 'melanogaster', 'used', 'within', 'known', 'study', 'identify', 'may', 'studies'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "both = top50_talks & top50_posters\n",
    "talks_only = top50_talks - top50_posters\n",
    "posters_only = top50_posters - top50_talks\n",
    "\n",
    "print('Common to both', both, '\\n')\n",
    "print('More common in talks', talks_only, '\\n')\n",
    "print('More common in posters', posters_only, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Basic classifyer\n",
    "NLTK contains a basic Baysean classification system. We can use that to see if something was a talk or poster favored word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "talk_num = 0\n",
    "poster_num = 0\n",
    "for x in abstracts:\n",
    "    if str(x) in assignments:\n",
    "        if assignments[str(x)]['type'] == 'talk':\n",
    "            documents.append((abstracts[x].split(),'talk'))\n",
    "            talk_num += 1\n",
    "        if assignments[str(x)]['type'] == 'poster':\n",
    "            documents.append((abstracts[x].split(),'poster'))\n",
    "            poster_num += 1\n",
    "total = poster_num + talk_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dist = nltk.FreqDist(full)\n",
    "\n",
    "# check the 200 most common words and keep all that are not stop words\n",
    "filtered_words = []\n",
    "for word in frequency_dist.most_common(200):\n",
    "    if (str.isalpha(word[0])) and (word[0].lower() not in stopwords):\n",
    "        filtered_words.append(word)\n",
    "        \n",
    "word_features =  [word_tuple[0] for word_tuple in filtered_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842\n"
     ]
    }
   ],
   "source": [
    "def document_features(document):    \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "random.shuffle(featuresets)\n",
    "print(len(featuresets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8111638954869359\n",
      "0.82\n",
      "Most Informative Features\n",
      "  contains(melanogaster) = True           poster : talk   =      2.5 : 1.0\n",
      "     contains(determine) = True           poster : talk   =      2.0 : 1.0\n",
      "       contains(nuclear) = True             talk : poster =      1.8 : 1.0\n",
      "    contains(previously) = True           poster : talk   =      1.8 : 1.0\n",
      "      contains(neuronal) = True             talk : poster =      1.8 : 1.0\n",
      "      contains(critical) = True             talk : poster =      1.7 : 1.0\n",
      "       contains(binding) = True             talk : poster =      1.6 : 1.0\n",
      "contains(transcriptional) = True             talk : poster =      1.6 : 1.0\n",
      "        contains(factor) = True             talk : poster =      1.6 : 1.0\n",
      "       contains(neurons) = True             talk : poster =      1.6 : 1.0\n",
      "          contains(find) = True             talk : poster =      1.5 : 1.0\n",
      "           contains(DNA) = True             talk : poster =      1.5 : 1.0\n",
      "       contains(defects) = True           poster : talk   =      1.5 : 1.0\n",
      "       contains(Medical) = True             talk : poster =      1.5 : 1.0\n",
      "       contains(pathway) = True             talk : poster =      1.5 : 1.0\n",
      "         contains(study) = True           poster : talk   =      1.5 : 1.0\n",
      "           contains(new) = True             talk : poster =      1.5 : 1.0\n",
      "          contains(show) = True             talk : poster =      1.5 : 1.0\n",
      "      contains(specific) = True             talk : poster =      1.5 : 1.0\n",
      "        contains(growth) = True             talk : poster =      1.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(poster_num/total)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "\n",
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like NTLK is doing a good job of predicting talks and posters, but on closer inspection it is barely beating the ratio of posters / total submissions. There is at best a 1% increase in the chance of getting a talk by following the recomendations of the classifier here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Clustering the abstracts\n",
    "I'd like to be able to make a un-supervised clustering of the abstracts to answer some questions:  \n",
    "1) Do the abstracts cluster based on section or super-section?  \n",
    "2) Are there clusters that are not represented by the sections  \n",
    "3) Are there individuals who maybe submitted their abstract to the wrong section?  \n",
    "  \n",
    "Going to follow a tutorial that I found first: http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep\n",
    "We need several lists for this analysis.  \n",
    "1) A list of the abstract numbers  \n",
    "2) A list of the abstracts  \n",
    "3) A list of the categories  / super categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = []\n",
    "texts = []\n",
    "categories = []\n",
    "for ab in abstracts:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
